{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5777e64",
   "metadata": {},
   "source": [
    "## DOC Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f081d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Introduction \\nMicrosoft Azure provides a broad set of GPU-accelerated virtual machines optimized for AI \\ntraining, inference, graphics rendering, HPC, and data processing. GPU families on Azure \\ninclude the NC, ND, NV , and NCads/NDas/Lsv3 variants, covering a wide range of NVIDIA \\nGPUs such as A100, H100, L40S, A10, T4, and previous generations. \\nAzure’s GPU portfolio is designed to meet different levels of performance, memory \\nrequirements, scalability, and cost, making it suitable for LLM training, fine-tuning, high-\\nthroughput inference, and visualization workloads. \\n2. GPU VM Families on Azure \\n2.1 NC Series – Compute-Optimized for AI Training \\n• Best for: Deep learning training, scientific computing. \\n• GPU models: NVIDIA A100 80GB, V100, K80. \\n• Examples: \\no Standard_NC24ads_A100_v4 – 1× A100 80GB. \\no Standard_NC96ads_A100_v4 – 4× A100 80GB (multi-GPU node). \\n• Use cases: \\no Transformer/LLM training (up to 70B parameters). \\no Computer vision training. \\no Reinforcement learning. \\n2.2 ND Series – Multi-GPU AI Training & Large Model Workloads \\n• Best for: Scalable multi-node distributed model training. \\n• GPU models: NVIDIA A100 80GB, H100 (ND H100 preview). \\n• Examples: \\no Standard_ND96asr_A100_v4 – 8 GPUs A100 80GB. \\no Standard_ND96isr_H100_v5 (preview) – 8 GPUs H100. \\n• Key features: \\no NVLink connection between GPUs. \\no High-bandwidth InfiniBand HDR. \\n \\n2.3 NV Series – Visualization & Rendering \\n• Best for: Graphics rendering, visualization, remote desktops. \\n• GPU models: NVIDIA L40S, RTX A6000, M60, T4. \\n• Examples: \\no Standard_NVadsA10_v5 – 1× A10 GPU. \\no Standard_NV32as_v4 – 1× L40S GPU. \\n• Use cases: \\no 3D visualization. \\no CAD workloads.\\n\\x0co Media encoding. \\n2.4 Other GPU-Accelerated Instances \\n• Lsv3 – High-performance local NVMe storage for data pipelines. \\n• HB/HC Series – Not GPU-based, but relevant for HPC workloads involving CPU-\\nheavy simulations. \\n3. Azure GPU Pricing Overview \\nAzure GPU pricing varies by: \\n• Region \\n• GPU type \\n• VM family \\n• CPU/RAM configuration \\n• Networking capabilities \\n• Purchase model (Pay-As-You-Go, Reserved Instances, Spot) \\nA100 80GB \\n• Standard_NC24ads_A100_v4: $3.40/hour \\n• Standard_ND96asr_A100_v4: $28.50/hour (8 GPUs) \\nL40S \\n• Standard_NV24as_v4: $2.10/hour \\nA10 \\n• Standard_NVadsA10_v5: $1.40/hour \\nT4 \\n• Standard_NC4as_T4_v3: $0.85/hour \\n4. Purchase Models & Cost Optimization \\n4.1 Pay-As-You-Go \\n• No commitment. \\n• Highest hourly cost. \\n• Good for POCs or infrequent training jobs. \\n4.2 Reserved Instances (1 or 3 years) \\n• Up to 65% cheaper than PAYG. \\n• Best for long-running production workloads. \\n4.3 Spot VMs \\n• Up to 90% cheaper.\\n\\x0c• VM may be evicted at any moment. \\n• Suitable for: \\no Long-running but restart-tolerant jobs. \\no Ray/TensorFlow/PyTorch distributed training that can checkpoint. \\n4.4 Savings Plans \\n• Flexible commitment to spend amount per hour. \\n• Applies across multiple VM families. \\n5. Networking Considerations \\n5.1 High-Performance Networking \\n• ND and NC series support: \\no InfiniBand HDR networking \\no NCCL-optimized GPU communication \\n• Essential for: \\no Multi-GPU distributed training (Data Parallel and Tensor Parallel) \\no Multi-node clusters for LLMs >70B \\n5.2 Bandwidth Requirements for Large Models \\n• Model parallel training requires: \\no >= 200 Gbps GPU-to-GPU interconnect. \\n• When using A100/H100 with NVLink: \\no Inter-node latency reduced significantly. \\n6. Storage Options \\n6.1 Managed Disks \\n• Premium SSD recommended for high IOPS. \\n6.2 Azure Blob Storage \\n• Ideal for storing large datasets and checkpoints. \\n• Supports: \\no Multi-part uploads \\no High throughput \\no Lifecycle policies for reducing cost \\n6.3 Ephemeral OS Disks \\n• Faster start times. \\n• Good for ephemeral workloads. \\n6.4 Local NVMe Storage (Lsv3 Series) \\n• High bandwidth.\\n\\x0c• Best for temporary data processing or vector DB storage. \\n7. Recommended Workload Profiles \\n7.1 Large Language Model Training \\n• Recommended GPUs: A100 80GB, H100 \\n• Node types: ND A100 v4 or ND H100 v5 \\n• Parallel methods: \\no Data Parallelism \\no Tensor Parallelism \\no Pipeline Parallelism \\n• Datasets: \\no Must be streamed from Blob or mounted via Azure Files. \\n7.2 LLM Fine-Tuning \\n• GPUs: A100 40GB/80GB, A10, L40S \\n• Batch size often limited by VRAM. \\n• Parameter-efficient tuning (LoRA/QLoRA) reduces cost. \\n7.3 High-Throughput Inference \\n• GPUs: L40S, A10, T4 \\n• Recommended VM families: \\no NVadsA10_v5 \\no NCas_T4_v3 \\n• Best for: \\no Chatbots \\no Embedding generation \\no Vision inference \\n7.4 Graphics & Rendering \\n• GPUs: L40S, A6000, A10 \\n• NV series ideal. \\n8. Region Comparison \\nRegion Avg GPU Cost Availability Notes \\nEast US Medium High Best for training workloads. \\nWest US 2 High Medium Higher demand → higher price. \\nCentral US Low Medium Often best price/performance. \\nWest Europe Medium-High High Good for compliance workloads. \\nSoutheast Asia High Low Limited A100 availability.\\n\\x0c9. Benchmark Notes (High-Level) \\nTraining Performance \\n• A100 80GB achieves: \\no 320 TFLOPS Tensor performance. \\no 40–60% faster LLM training vs V100. \\n• H100 (preview) achieves: \\no Up to 3× faster LLM throughput vs A100. \\nInference Performance  \\n• L40S offers: \\no 6× throughput of T4 for vision tasks. \\n• A10 is: \\no 2× faster than T4 for transformer inference. \\n10. Key Limitations \\n• Some regions lack A100/H100 availability. \\n• Spot VMs may be terminated frequently. \\n• Large multi-node clusters may require quota increases. \\n• Peak times may cause provisioning delays. \\n• NV series not recommended for deep learning training. \\n11. FinOps Recommendations \\n• Use Spot VMs for: \\no Non-urgent training. \\no Batch inference pipelines. \\n• Use Reserved Instances for: \\no Always-on inference clusters. \\n• Enable: \\no Automatic checkpointing. \\no Autoscaling with GPU utilization metrics. \\n• Monitor with: \\no Azure Monitor \\no Application Insights \\no Prometheus/Grafana \\n12. Scaling Strategies \\nHorizontal Scaling \\n• Add more GPU nodes. \\n• Useful for embedding generation and distributed inference.\\n\\x0cVertical Scaling \\n• Switch to a larger VM with more VRAM. \\n• Useful for training larger models. \\nMulti-Node ML Training \\n• Use Azure Machine Learning (AzureML): \\no Orchestrates distributed PyTorch jobs. \\no Supports DeepSpeed, HuggingFace Accelerate, Ray. \\n13. Use-Case Mapping Summary \\nUse Case Recommended GPU VM Series Cost Level \\nLLM Training (Large) A100/H100 ND v4/v5 Very High \\nFine-Tuning A100/A10/L40S NC/NV Medium \\nReal-Time Inference L40S/T4 NV/NC Low–Medium \\nVision Inference L40S/A10 NV Medium \\nRendering L40S/A6000 NV Medium \\nCAD/Visualization A10/M60 NV Low–Medium \\n14. Conclusion \\nAzure provides one of the most flexible and scalable GPU environments for AI workloads. \\nFrom small inference deployments to massive LLM training clusters, users can choose \\nbetween A10, L40S, A100, and H100 GPUs, adjusting compute, networking, and storage \\noptions to match performance and cost. \\nFor optimal results: \\n• Choose the VM family based on workload type. \\n• Use Spot VMs when possible. \\n• Scale with AzureML for distributed training. \\n• Use FinOps principles to monitor and control spend. \\n• Pick regions based on availability and pricing.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "file_path = \"./azure-doc.pdf\"\n",
    "loader = PyPDFLoader(file_path, mode=\"single\")\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05d71e",
   "metadata": {},
   "source": [
    "## DOC Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be44d17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[r\"\\r?\\n(?=\\d+\\.\\s+)\"],\n",
    "    is_separator_regex=True,\n",
    "    chunk_size=1,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "172a05ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='1. Introduction \\nMicrosoft Azure provides a broad set of GPU-accelerated virtual machines optimized for AI \\ntraining, inference, graphics rendering, HPC, and data processing. GPU families on Azure \\ninclude the NC, ND, NV , and NCads/NDas/Lsv3 variants, covering a wide range of NVIDIA \\nGPUs such as A100, H100, L40S, A10, T4, and previous generations. \\nAzure’s GPU portfolio is designed to meet different levels of performance, memory \\nrequirements, scalability, and cost, making it suitable for LLM training, fine-tuning, high-\\nthroughput inference, and visualization workloads. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n2. GPU VM Families on Azure \\n2.1 NC Series – Compute-Optimized for AI Training \\n• Best for: Deep learning training, scientific computing. \\n• GPU models: NVIDIA A100 80GB, V100, K80. \\n• Examples: \\no Standard_NC24ads_A100_v4 – 1× A100 80GB. \\no Standard_NC96ads_A100_v4 – 4× A100 80GB (multi-GPU node). \\n• Use cases: \\no Transformer/LLM training (up to 70B parameters). \\no Computer vision training. \\no Reinforcement learning. \\n2.2 ND Series – Multi-GPU AI Training & Large Model Workloads \\n• Best for: Scalable multi-node distributed model training. \\n• GPU models: NVIDIA A100 80GB, H100 (ND H100 preview). \\n• Examples: \\no Standard_ND96asr_A100_v4 – 8 GPUs A100 80GB. \\no Standard_ND96isr_H100_v5 (preview) – 8 GPUs H100. \\n• Key features: \\no NVLink connection between GPUs. \\no High-bandwidth InfiniBand HDR. \\n \\n2.3 NV Series – Visualization & Rendering \\n• Best for: Graphics rendering, visualization, remote desktops. \\n• GPU models: NVIDIA L40S, RTX A6000, M60, T4. \\n• Examples: \\no Standard_NVadsA10_v5 – 1× A10 GPU. \\no Standard_NV32as_v4 – 1× L40S GPU. \\n• Use cases: \\no 3D visualization. \\no CAD workloads.\\n\\x0co Media encoding. \\n2.4 Other GPU-Accelerated Instances \\n• Lsv3 – High-performance local NVMe storage for data pipelines. \\n• HB/HC Series – Not GPU-based, but relevant for HPC workloads involving CPU-\\nheavy simulations. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n3. Azure GPU Pricing Overview \\nAzure GPU pricing varies by: \\n• Region \\n• GPU type \\n• VM family \\n• CPU/RAM configuration \\n• Networking capabilities \\n• Purchase model (Pay-As-You-Go, Reserved Instances, Spot) \\nA100 80GB \\n• Standard_NC24ads_A100_v4: $3.40/hour \\n• Standard_ND96asr_A100_v4: $28.50/hour (8 GPUs) \\nL40S \\n• Standard_NV24as_v4: $2.10/hour \\nA10 \\n• Standard_NVadsA10_v5: $1.40/hour \\nT4 \\n• Standard_NC4as_T4_v3: $0.85/hour '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n4. Purchase Models & Cost Optimization \\n4.1 Pay-As-You-Go \\n• No commitment. \\n• Highest hourly cost. \\n• Good for POCs or infrequent training jobs. \\n4.2 Reserved Instances (1 or 3 years) \\n• Up to 65% cheaper than PAYG. \\n• Best for long-running production workloads. \\n4.3 Spot VMs \\n• Up to 90% cheaper.\\n\\x0c• VM may be evicted at any moment. \\n• Suitable for: \\no Long-running but restart-tolerant jobs. \\no Ray/TensorFlow/PyTorch distributed training that can checkpoint. \\n4.4 Savings Plans \\n• Flexible commitment to spend amount per hour. \\n• Applies across multiple VM families. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n5. Networking Considerations \\n5.1 High-Performance Networking \\n• ND and NC series support: \\no InfiniBand HDR networking \\no NCCL-optimized GPU communication \\n• Essential for: \\no Multi-GPU distributed training (Data Parallel and Tensor Parallel) \\no Multi-node clusters for LLMs >70B \\n5.2 Bandwidth Requirements for Large Models \\n• Model parallel training requires: \\no >= 200 Gbps GPU-to-GPU interconnect. \\n• When using A100/H100 with NVLink: \\no Inter-node latency reduced significantly. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n6. Storage Options \\n6.1 Managed Disks \\n• Premium SSD recommended for high IOPS. \\n6.2 Azure Blob Storage \\n• Ideal for storing large datasets and checkpoints. \\n• Supports: \\no Multi-part uploads \\no High throughput \\no Lifecycle policies for reducing cost \\n6.3 Ephemeral OS Disks \\n• Faster start times. \\n• Good for ephemeral workloads. \\n6.4 Local NVMe Storage (Lsv3 Series) \\n• High bandwidth.\\n\\x0c• Best for temporary data processing or vector DB storage. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n7. Recommended Workload Profiles \\n7.1 Large Language Model Training \\n• Recommended GPUs: A100 80GB, H100 \\n• Node types: ND A100 v4 or ND H100 v5 \\n• Parallel methods: \\no Data Parallelism \\no Tensor Parallelism \\no Pipeline Parallelism \\n• Datasets: \\no Must be streamed from Blob or mounted via Azure Files. \\n7.2 LLM Fine-Tuning \\n• GPUs: A100 40GB/80GB, A10, L40S \\n• Batch size often limited by VRAM. \\n• Parameter-efficient tuning (LoRA/QLoRA) reduces cost. \\n7.3 High-Throughput Inference \\n• GPUs: L40S, A10, T4 \\n• Recommended VM families: \\no NVadsA10_v5 \\no NCas_T4_v3 \\n• Best for: \\no Chatbots \\no Embedding generation \\no Vision inference \\n7.4 Graphics & Rendering \\n• GPUs: L40S, A6000, A10 \\n• NV series ideal. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n8. Region Comparison \\nRegion Avg GPU Cost Availability Notes \\nEast US Medium High Best for training workloads. \\nWest US 2 High Medium Higher demand → higher price. \\nCentral US Low Medium Often best price/performance. \\nWest Europe Medium-High High Good for compliance workloads. \\nSoutheast Asia High Low Limited A100 availability.\\n\\x0c9. Benchmark Notes (High-Level) \\nTraining Performance \\n• A100 80GB achieves: \\no 320 TFLOPS Tensor performance. \\no 40–60% faster LLM training vs V100. \\n• H100 (preview) achieves: \\no Up to 3× faster LLM throughput vs A100. \\nInference Performance  \\n• L40S offers: \\no 6× throughput of T4 for vision tasks. \\n• A10 is: \\no 2× faster than T4 for transformer inference. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n10. Key Limitations \\n• Some regions lack A100/H100 availability. \\n• Spot VMs may be terminated frequently. \\n• Large multi-node clusters may require quota increases. \\n• Peak times may cause provisioning delays. \\n• NV series not recommended for deep learning training. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n11. FinOps Recommendations \\n• Use Spot VMs for: \\no Non-urgent training. \\no Batch inference pipelines. \\n• Use Reserved Instances for: \\no Always-on inference clusters. \\n• Enable: \\no Automatic checkpointing. \\no Autoscaling with GPU utilization metrics. \\n• Monitor with: \\no Azure Monitor \\no Application Insights \\no Prometheus/Grafana '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n12. Scaling Strategies \\nHorizontal Scaling \\n• Add more GPU nodes. \\n• Useful for embedding generation and distributed inference.\\n\\x0cVertical Scaling \\n• Switch to a larger VM with more VRAM. \\n• Useful for training larger models. \\nMulti-Node ML Training \\n• Use Azure Machine Learning (AzureML): \\no Orchestrates distributed PyTorch jobs. \\no Supports DeepSpeed, HuggingFace Accelerate, Ray. '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n13. Use-Case Mapping Summary \\nUse Case Recommended GPU VM Series Cost Level \\nLLM Training (Large) A100/H100 ND v4/v5 Very High \\nFine-Tuning A100/A10/L40S NC/NV Medium \\nReal-Time Inference L40S/T4 NV/NC Low–Medium \\nVision Inference L40S/A10 NV Medium \\nRendering L40S/A6000 NV Medium \\nCAD/Visualization A10/M60 NV Low–Medium '),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'moddate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'total_pages': 6}, page_content='\\n14. Conclusion \\nAzure provides one of the most flexible and scalable GPU environments for AI workloads. \\nFrom small inference deployments to massive LLM training clusters, users can choose \\nbetween A10, L40S, A100, and H100 GPUs, adjusting compute, networking, and storage \\noptions to match performance and cost. \\nFor optimal results: \\n• Choose the VM family based on workload type. \\n• Use Spot VMs when possible. \\n• Scale with AzureML for distributed training. \\n• Use FinOps principles to monitor and control spend. \\n• Pick regions based on availability and pricing.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63247a",
   "metadata": {},
   "source": [
    "## Embeddings and Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4cf00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,          # your 13 docs or chunks\n",
    "    embedding=embeddings,          # your OpenAIEmbeddings instance\n",
    "    persist_directory=\"../app/data/vec_database/chroma_db\"  # folder to persist locally\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad387e6",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"content\": \"...\",\n",
    "    \"metadata\": {\n",
    "        \"creator\": \"Microsoft® Word LTSC\",\n",
    "        \"moddate\": \"2025-11-15T12:10:12-03:00\",\n",
    "        \"total_pages\": 6,\n",
    "        \"creationdate\": \"2025-11-15T12:10:12-03:00\",\n",
    "        \"source\": \"./azure-doc.pdf\",\n",
    "        \"producer\": \"Microsoft® Word LTSC\",\n",
    "        \"author\": \"Emerson Faria\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33e2ed16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'moddate': '2025-11-17T23:51:16-03:00', 'producer': 'Microsoft® Word LTSC', 'source': './azure-doc.pdf', 'total_pages': 6, 'author': 'Emerson Faria', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00'}, page_content='\\n3. Azure GPU Pricing Overview \\nAzure GPU pricing varies by: \\n• Region \\n• GPU type \\n• VM family \\n• CPU/RAM configuration \\n• Networking capabilities \\n• Purchase model (Pay-As-You-Go, Reserved Instances, Spot) \\nA100 80GB \\n• Standard_NC24ads_A100_v4: $3.40/hour \\n• Standard_ND96asr_A100_v4: $28.50/hour (8 GPUs) \\nL40S \\n• Standard_NV24as_v4: $2.10/hour \\nA10 \\n• Standard_NVadsA10_v5: $1.40/hour \\nT4 \\n• Standard_NC4as_T4_v3: $0.85/hour '),\n",
       "  0.6779065344218039),\n",
       " (Document(metadata={'source': './azure-doc.pdf', 'moddate': '2025-11-17T23:51:16-03:00', 'total_pages': 6, 'producer': 'Microsoft® Word LTSC', 'author': 'Emerson Faria', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00'}, page_content='1. Introduction \\nMicrosoft Azure provides a broad set of GPU-accelerated virtual machines optimized for AI \\ntraining, inference, graphics rendering, HPC, and data processing. GPU families on Azure \\ninclude the NC, ND, NV , and NCads/NDas/Lsv3 variants, covering a wide range of NVIDIA \\nGPUs such as A100, H100, L40S, A10, T4, and previous generations. \\nAzure’s GPU portfolio is designed to meet different levels of performance, memory \\nrequirements, scalability, and cost, making it suitable for LLM training, fine-tuning, high-\\nthroughput inference, and visualization workloads. '),\n",
       "  0.42848612108766926),\n",
       " (Document(metadata={'producer': 'Microsoft® Word LTSC', 'creationdate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'moddate': '2025-11-17T23:51:16-03:00', 'author': 'Emerson Faria', 'creator': 'Microsoft® Word LTSC', 'total_pages': 6}, page_content='\\n14. Conclusion \\nAzure provides one of the most flexible and scalable GPU environments for AI workloads. \\nFrom small inference deployments to massive LLM training clusters, users can choose \\nbetween A10, L40S, A100, and H100 GPUs, adjusting compute, networking, and storage \\noptions to match performance and cost. \\nFor optimal results: \\n• Choose the VM family based on workload type. \\n• Use Spot VMs when possible. \\n• Scale with AzureML for distributed training. \\n• Use FinOps principles to monitor and control spend. \\n• Pick regions based on availability and pricing.'),\n",
       "  0.4264722603721217),\n",
       " (Document(metadata={'moddate': '2025-11-17T23:51:16-03:00', 'creationdate': '2025-11-17T23:51:16-03:00', 'source': './azure-doc.pdf', 'creator': 'Microsoft® Word LTSC', 'author': 'Emerson Faria', 'total_pages': 6, 'producer': 'Microsoft® Word LTSC'}, page_content='\\n2. GPU VM Families on Azure \\n2.1 NC Series – Compute-Optimized for AI Training \\n• Best for: Deep learning training, scientific computing. \\n• GPU models: NVIDIA A100 80GB, V100, K80. \\n• Examples: \\no Standard_NC24ads_A100_v4 – 1× A100 80GB. \\no Standard_NC96ads_A100_v4 – 4× A100 80GB (multi-GPU node). \\n• Use cases: \\no Transformer/LLM training (up to 70B parameters). \\no Computer vision training. \\no Reinforcement learning. \\n2.2 ND Series – Multi-GPU AI Training & Large Model Workloads \\n• Best for: Scalable multi-node distributed model training. \\n• GPU models: NVIDIA A100 80GB, H100 (ND H100 preview). \\n• Examples: \\no Standard_ND96asr_A100_v4 – 8 GPUs A100 80GB. \\no Standard_ND96isr_H100_v5 (preview) – 8 GPUs H100. \\n• Key features: \\no NVLink connection between GPUs. \\no High-bandwidth InfiniBand HDR. \\n \\n2.3 NV Series – Visualization & Rendering \\n• Best for: Graphics rendering, visualization, remote desktops. \\n• GPU models: NVIDIA L40S, RTX A6000, M60, T4. \\n• Examples: \\no Standard_NVadsA10_v5 – 1× A10 GPU. \\no Standard_NV32as_v4 – 1× L40S GPU. \\n• Use cases: \\no 3D visualization. \\no CAD workloads.\\n\\x0co Media encoding. \\n2.4 Other GPU-Accelerated Instances \\n• Lsv3 – High-performance local NVMe storage for data pipelines. \\n• HB/HC Series – Not GPU-based, but relevant for HPC workloads involving CPU-\\nheavy simulations. '),\n",
       "  0.4065405520959864)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._similarity_search_with_relevance_scores(\"Azure GPU Pricing Overview\", k=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
